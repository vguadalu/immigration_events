{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration Events\n",
    "\n",
    "#### Project Summary\n",
    "The objective of this project is to create an ETL pipeline for I94 immigration, global land temperatures and US demographics datasets to form an analytics database on immigration events.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Clean the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import udf, trim, col, when, initcap, dayofmonth, dayofweek, month, year, weekofyear, date_format\n",
    "\n",
    "import etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "To create the analytics database, the following steps will be carried out:\n",
    "\n",
    "* Use Spark to load the data into dataframes.\n",
    "* Perform data cleaning functions on all the datasets.\n",
    "* Create i94 port code database from valid_i94_ports data set.\n",
    "* Create immigration arrival date dimension table from I94 immigration dataset.\n",
    "* Create immigration departure date dimension table from I94 immigration dataset.\n",
    "* Create immigrant dimension table from the I94 immigration dataset. \n",
    "* Create usa demographics dimension table from the us cities demographics data.\n",
    "* Create fact table from the clean I94 immigration dataset and the i94 ports dataframe.\n",
    "\n",
    "The technology used in this project is Apache Spark. Data will be read and staged using Spark.\n",
    "\n",
    "#### Describe Data \n",
    "The data being used is i94 immigration data from the US National Tourism and Trade Office, World Temperature Data from Kaggle, and a U.S. City Demographic Data from OpenSoft. To supplement the i94 immigration data valid i94_ports and i94_addr codes datasets were created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Clean the Data\n",
    "#### Explore the Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_file = '/data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat'\n",
    "temperature_file = '/data2/GlobalLandTemperaturesByCity.csv'\n",
    "demographics_file = 'us-cities-demographics.csv'\n",
    "i94port_file = 'valid_i94_ports.csv'\n",
    "i94addr_file = 'valid_i94addr_codes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df =spark.read.format('com.github.saurfang.sas.spark').load(immigration_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| 46.0|2016.0|  12.0| 129.0| 129.0|    HOU|20789.0|    1.0|     TX|20802.0|  46.0|    1.0|  1.0|20161201|     MDD| null|      H|      O|   null|      M| 1970.0|05262018|     M|  null|     RS| 9.755413803E10| 7715|      E2|\n",
      "| 56.0|2016.0|  12.0| 245.0| 245.0|    NEW|20789.0|    1.0|     OH|20835.0|  28.0|    3.0|  1.0|20161201|     BEJ| null|      U|      O|   null|      M| 1988.0|     D/S|     F|  null|     CA| 9.062371823E10|  819|      F1|\n",
      "| 67.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     MD|20794.0|  48.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1968.0|06012017|     M|  5920|   null|8.0105031527E10| null|      B2|\n",
      "| 68.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     FL|20792.0|  46.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1970.0|06012017|     F|  5920|   null|8.0105106727E10| null|      B2|\n",
      "| 69.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     HI|20792.0|  48.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1968.0|06012017|     M|  5920|   null|8.0105107627E10| null|      B2|\n",
      "| 70.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     FL|20792.0|   9.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 2007.0|06012017|     F|  5920|   null|8.0105108527E10| null|      B2|\n",
      "| 71.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     VA|20792.0|  34.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1982.0|06012017|     F|  5920|   null|8.0105152527E10| null|      B2|\n",
      "| 72.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     NY|20792.0|  69.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1947.0|06012017|     F|  5920|   null|8.0105153427E10| null|      B2|\n",
      "| 73.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     NY|20794.0|  66.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1950.0|06012017|     F|  5920|   null|8.0105155227E10| null|      B2|\n",
      "| 74.0|2016.0|  12.0| 512.0| 512.0|    XXX|20789.0|    2.0|     DC|20794.0|  56.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1960.0|06012017|     M|  null|   null|8.0105156127E10| null|      B2|\n",
      "| 75.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     VA|20794.0|   6.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 2010.0|06012017|     M|  5920|   null|8.0105189127E10| null|      B2|\n",
      "| 76.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    2.0|     VA|20794.0|  38.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1978.0|06012017|     U|  5920|   null|8.0105190027E10| null|      B2|\n",
      "| 77.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    9.0|     NY|20794.0|  40.0|    2.0|  1.0|20161201|     NAS| null|      A|      D|   null|      M| 1976.0|06012017|     M|  5920|   null|8.0105191027E10| null|      B2|\n",
      "| 78.0|2016.0|  12.0| 512.0| 512.0|    PEV|20789.0|    9.0|     FL|20792.0|  28.0|    2.0|  1.0|20161201|    null| null|      A|      D|   null|      M| 1988.0|06012017|     F|  5920|   null|8.0105217627E10| null|      B2|\n",
      "|106.0|2016.0|  12.0| 104.0| 104.0|    CHM|20789.0|    9.0|     NJ|   null|  26.0|    2.0|  1.0|20161201|    null| null|      A|   null|   null|   null| 1990.0|02282017|     F|  5319|   null|8.6530066528E10| null|      WT|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df.show(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = spark.read.format('csv').option('header', 'true').load(temperature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|              6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01| 5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|             10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-09-01| 12.780999999999999|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-10-01|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-11-01|  4.638999999999999|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-12-01|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-01-01|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.show(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df = spark.read.option('delimiter', ';').format('csv').option('header', 'true').load(demographics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df.show(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94port_df = spark.read.format('csv').option('header', 'true').load('valid_i94_ports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+-----------+\n",
      "|i94port|                City|      State Code|        _c3|\n",
      "+-------+--------------------+----------------+-----------+\n",
      "|    ALC|               ALCAN| AK             |       null|\n",
      "|    ANC|           ANCHORAGE|     AK         |       null|\n",
      "|    BAR|BAKER AAF - BAKER...|              AK|       null|\n",
      "|    DAC|       DALTONS CACHE|         AK     |       null|\n",
      "|    PIZ|DEW STATION PT LA...|              AK|       null|\n",
      "|    DTH|        DUTCH HARBOR|        AK      |       null|\n",
      "|    EGL|               EAGLE| AK             |       null|\n",
      "|    FRB|           FAIRBANKS|     AK         |       null|\n",
      "|    HOM|               HOMER| AK             |           |\n",
      "|    HYD|               HYDER| AK             |       null|\n",
      "|    JUN|              JUNEAU|  AK            |       null|\n",
      "|    5KE|           KETCHIKAN|              AK|       null|\n",
      "|    KET|           KETCHIKAN|     AK         |       null|\n",
      "|    MOS|MOSES POINT INTER...|              AK|       null|\n",
      "|    NIK|             NIKISKI|   AK           |       null|\n",
      "+-------+--------------------+----------------+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df.show(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94addr_df = spark.read.format('csv').option('header', 'true').load('valid_i94addr_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94addr_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|i94addr|            State|\n",
      "+-------+-----------------+\n",
      "|     AL|          ALABAMA|\n",
      "|     AK|           ALASKA|\n",
      "|     AZ|          ARIZONA|\n",
      "|     AR|         ARKANSAS|\n",
      "|     CA|       CALIFORNIA|\n",
      "|     CO|         COLORADO|\n",
      "|     CT|      CONNECTICUT|\n",
      "|     DE|         DELAWARE|\n",
      "|     DC|DIST. OF COLUMBIA|\n",
      "|     FL|          FLORIDA|\n",
      "|     GA|          GEORGIA|\n",
      "|     GU|             GUAM|\n",
      "|     HI|           HAWAII|\n",
      "|     ID|            IDAHO|\n",
      "|     IL|         ILLINOIS|\n",
      "+-------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94addr_df.show(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "\n",
    "Based on looking at the different datasets, the cleaning steps for all datasets should be to remove columns that seem to have a vast majority of null values and remove duplicate rows. For each dataset there will also be addtional clean up steps that will be taken to ensure the data fits our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i94port dataset must remove the \"_c3\" column since it does not have any valueable information for our analysis. All null values are replaced by the value 'N/A', to symbolize that the information is not available. Also the \"State Code\" column has varying amounts of empty space that needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_i94port_data(i94port_df):\n",
    "    \"\"\" Cleans i94port code dataset\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    i94port_df: dataframe containing the valid i94port values\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    i94port_df = spark dataframe \n",
    "    \"\"\"    \n",
    "    i94port_df = i94port_df.drop(i94port_df._c3)\n",
    "    i94port_df = i94port_df.withColumn('City', initcap(col('City')))\n",
    "    i94port_df = i94port_df.fillna('N/A')\n",
    "    remove_space_udf = udf(lambda s: s.replace(\" \", \"\"), T.StringType())\n",
    "    i94port_df = i94port_df.withColumn('State Code', remove_space_udf(col('State Code')))\n",
    "    \n",
    "    return i94port_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n",
      "|i94port|                City|State Code|\n",
      "+-------+--------------------+----------+\n",
      "|    ALC|               Alcan|        AK|\n",
      "|    ANC|           Anchorage|        AK|\n",
      "|    BAR|Baker Aaf - Baker...|        AK|\n",
      "|    DAC|       Daltons Cache|        AK|\n",
      "|    PIZ|Dew Station Pt La...|        AK|\n",
      "|    DTH|        Dutch Harbor|        AK|\n",
      "|    EGL|               Eagle|        AK|\n",
      "|    FRB|           Fairbanks|        AK|\n",
      "|    HOM|               Homer|        AK|\n",
      "|    HYD|               Hyder|        AK|\n",
      "+-------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df = clean_i94port_data(i94port_df)\n",
    "i94port_df.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the i94 immigration dataset, i94port and i94addr rows that do not match values in the valid i94port/i94addr codes datset will be removed by doing left joins. All \"arrdate\" and \"depdate\" rows with \"NaN\" values will be removed.\n",
    "\n",
    "Once the data has been cleaned, some value modifications will be performed to better suit out analysis. The visa value will be converted from a number representation to its true value taken from the i94 immigration labels descriptions. The \"arrdate\" and \"depdate\" will both be converted from second since epoch to Y-M-D format for easier understandability. All null values in \"entdepu\" have been replaced with \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_immigration_data(og_immigration_df, i94port_df, i94addr_df):\n",
    "    \"\"\" Removes rows with invalid values and columns\n",
    "    with a majority of null values.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    og_immigration_df: immigration dataframe as it comes from the file\n",
    "    i94port_df: dataframe containing the valid i94port values\n",
    "    i94addr_df: dataframe containing the valid i94addr values\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    immigration_df = cleaned dataframe \n",
    "    \"\"\"    \n",
    "    immigration_df = og_immigration_df.drop(og_immigration_df.fltno)\n",
    "    \n",
    "    immigration_df = immigration_df.join(i94addr_df, ['i94addr'], 'leftsemi')\n",
    "    immigration_df = immigration_df.join(i94port_df, ['i94port'], 'leftsemi')\n",
    "    \n",
    "    immigration_df = immigration_df.filter(immigration_df['arrdate'] != 'NaN')\n",
    "    immigration_df = immigration_df.filter(immigration_df['depdate'] != 'NaN')\n",
    "\n",
    "    return immigration_df\n",
    "\n",
    "def _convert_datetime(x):\n",
    "    start = datetime(1960, 1, 1)\n",
    "    return start + timedelta(days=int(x))\n",
    "\n",
    "def manipulate_immigration_data(immigration_df):\n",
    "    \"\"\" Updates values of some columns to facilitate analysis\n",
    "    Parameters\n",
    "    -----------\n",
    "    immigration_df: cleaned immigration dataframe \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    immigration_df = updated dataframe \n",
    "    \"\"\"\n",
    "    visa_value = when(immigration_df[\"i94visa\"] == 1,\n",
    "                        'Buisness').when(immigration_df['i94visa']==2,\n",
    "                                         'Pleasure').otherwise('Student')\n",
    "    immigration_df = immigration_df.withColumn('i94visa', visa_value)\n",
    "    \n",
    "    udf_datetime_convert = udf(lambda x: _convert_datetime(x), T.DateType())\n",
    "    immigration_df = immigration_df.withColumn('arrdate', \n",
    "                                               udf_datetime_convert('arrdate'))\n",
    "    immigration_df = immigration_df.withColumn('depdate', \n",
    "                                               udf_datetime_convert('depdate'))\n",
    "    immigration_df = immigration_df.fillna('N/A', subset=['entdepu'])\n",
    "    \n",
    "\n",
    "    return immigration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+------+------+------+------+----------+-------+----------+------+--------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+--------+\n",
      "|i94port|i94addr|cicid| i94yr|i94mon|i94cit|i94res|   arrdate|i94mode|   depdate|i94bir| i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|visatype|\n",
      "+-------+-------+-----+------+------+------+------+----------+-------+----------+------+--------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+--------+\n",
      "|    HOU|     TX| 46.0|2016.0|  12.0| 129.0| 129.0|2016-12-01|    1.0|2016-12-14|  46.0|Buisness|  1.0|20161201|     MDD| null|      H|      O|    N/A|      M| 1970.0|05262018|     M|  null|     RS| 9.755413803E10|      E2|\n",
      "|    NEW|     OH| 56.0|2016.0|  12.0| 245.0| 245.0|2016-12-01|    1.0|2017-01-16|  28.0| Student|  1.0|20161201|     BEJ| null|      U|      O|    N/A|      M| 1988.0|     D/S|     F|  null|     CA| 9.062371823E10|      F1|\n",
      "|    PEV|     MD| 67.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-06|  48.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 1968.0|06012017|     M|  5920|   null|8.0105031527E10|      B2|\n",
      "|    PEV|     FL| 68.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-04|  46.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 1970.0|06012017|     F|  5920|   null|8.0105106727E10|      B2|\n",
      "|    PEV|     HI| 69.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-04|  48.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 1968.0|06012017|     M|  5920|   null|8.0105107627E10|      B2|\n",
      "|    PEV|     FL| 70.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-04|   9.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 2007.0|06012017|     F|  5920|   null|8.0105108527E10|      B2|\n",
      "|    PEV|     VA| 71.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-04|  34.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 1982.0|06012017|     F|  5920|   null|8.0105152527E10|      B2|\n",
      "|    PEV|     NY| 72.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-04|  69.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 1947.0|06012017|     F|  5920|   null|8.0105153427E10|      B2|\n",
      "|    PEV|     NY| 73.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-06|  66.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 1950.0|06012017|     F|  5920|   null|8.0105155227E10|      B2|\n",
      "|    XXX|     DC| 74.0|2016.0|  12.0| 512.0| 512.0|2016-12-01|    2.0|2016-12-06|  56.0|Pleasure|  1.0|20161201|     NAS| null|      A|      D|    N/A|      M| 1960.0|06012017|     M|  null|   null|8.0105156127E10|      B2|\n",
      "+-------+-------+-----+------+------+------+------+----------+-------+----------+------+--------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_df = clean_immigration_data(immigration_df, i94port_df, i94addr_df)\n",
    "immigration_df = manipulate_immigration_data(immigration_df)\n",
    "immigration_df.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the temperature data set, we will only keep the rows that are for the United States and have a date in the 2000s which fit the scope of the analysys. The values in the \"City\" column will be capitalized to match how the value appears in the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_temperature_data(og_temperature_df):\n",
    "    \"\"\" Removes rows with duplicates, data is that irrelevant to our analysis\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    og_temperature_df: temp dataframe as it comes from the file\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    temperature_df = cleaned temperature dataframe \n",
    "    \"\"\"    \n",
    "    temperature_df = og_temperature_df.dropDuplicates(['dt','City', 'Country']) \n",
    "    temperature_df = temperature_df.filter(temperature_df.AverageTemperature.isNotNull())\n",
    "    temperature_df = temperature_df.filter(temperature_df.Country == 'United States')\n",
    "    temperature_df = temperature_df.withColumn('year', year('dt'))\n",
    "    temperature_df = temperature_df.filter(temperature_df.year.rlike('201|200'))\n",
    "    demographics_df = demographics_df.fillna('N/A', subset=['State Code'])\n",
    "    \n",
    "\n",
    "    return temperature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+----------------+-------------+--------+---------+----+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty|            City|      Country|Latitude|Longitude|year|\n",
      "+----------+-------------------+-----------------------------+----------------+-------------+--------+---------+----+\n",
      "|2000-01-01| 11.484000000000002|                        0.336|        Savannah|United States|  31.35N|   81.05W|2000|\n",
      "|2000-01-01|             10.081|                        0.228|       Sunnyvale|United States|  37.78N|  122.03W|2000|\n",
      "|2000-02-01|-0.8250000000000002|                        0.264|Sterling Heights|United States|  42.59N|   82.91W|2000|\n",
      "|2000-02-01|              4.659|          0.14800000000000002|          Topeka|United States|  39.38N|   95.72W|2000|\n",
      "|2000-02-01|             12.377|                        0.397|          Tucson|United States|  31.35N|  111.20W|2000|\n",
      "|2000-05-01| 25.316999999999997|                        0.249|        Beaumont|United States|  29.74N|   94.15W|2000|\n",
      "|2000-05-01|             19.073|                        0.317|        El Monte|United States|  34.56N|  118.70W|2000|\n",
      "|2000-05-01|             21.844|                        0.073|   Oklahoma City|United States|  36.17N|   97.46W|2000|\n",
      "|2000-05-01|             19.073|                        0.317|     West Covina|United States|  34.56N|  118.70W|2000|\n",
      "|2000-06-01|             19.724|                        0.282|       Fairfield|United States|  37.78N|  122.03W|2000|\n",
      "+----------+-------------------+-----------------------------+----------------+-------------+--------+---------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_df = clean_temperature_data(temp_df)\n",
    "temperature_df.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the US city demogrpahics dataset, the \"City\" column is capitalized and rows with null values are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_demographics_data(demographics_df):\n",
    "    \"\"\" Cleans demographics dataset\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    demographics_df: city demogrpahics dataframe as it comes from the file\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    demographics_df = cleaned demogrpahics dataframe \n",
    "    \"\"\"    \n",
    "    demographics_df = demographics_df.dropDuplicates(['City', 'State', 'State Code'])\n",
    "    demographics_df = demographics_df.filter(demographics_df.City.isNotNull())\n",
    "    \n",
    "    return demographics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|          City|      State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n",
      "+--------------+-----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|  Delray Beach|    Florida|      47.9|          32219|            34042|           66261|              4232|       16639|                  2.35|        FL|               Asian|  1696|\n",
      "|   Jersey City| New Jersey|      34.3|         131765|           132512|          264277|              4374|      109186|                  2.57|        NJ|  Hispanic or Latino| 79718|\n",
      "|     Rockville|   Maryland|      38.1|          31205|            35793|           66998|              1990|       25047|                   2.6|        MD|American Indian a...|   594|\n",
      "|      Alhambra| California|      41.0|          42184|            43388|           85572|              1673|       44441|                  2.89|        CA|American Indian a...|   687|\n",
      "|    Cincinnati|       Ohio|      32.7|         143654|           154883|          298537|             13699|       16896|                  2.08|        OH|               White|162245|\n",
      "|      Gulfport|Mississippi|      35.1|          33108|            38764|           71872|              6646|        3072|                  2.54|        MS|Black or African-...| 27799|\n",
      "|  South Jordan|       Utah|      33.8|          33669|            32970|           66639|              2595|        6142|                  3.61|        UT|               White| 59046|\n",
      "|Urban Honolulu|     Hawaii|      41.4|         176807|           175959|          352766|             23213|      101312|                  2.69|        HI|               Asian|240978|\n",
      "|    Boca Raton|    Florida|      47.3|          44760|            48466|           93226|              4367|       21117|                  2.22|        FL|               White| 80781|\n",
      "|        Caguas|Puerto Rico|      40.4|          34743|            42265|           77008|              null|        null|                  null|        PR|  Hispanic or Latino| 76349|\n",
      "+--------------+-----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_df = clean_demographics_data(demographics_df)\n",
    "demographics_df.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "<img src=\"Immigrations_Model.png\">\n",
    "\n",
    "#### 3.2 Data Pipeline\n",
    "The steps necessary to create this data pipeline are:\n",
    "* Load the datasets\n",
    "* Clean the datasets to better suit the analysis\n",
    "* Create immigrations table to be the fact table using data from the i94 immigration data set and the i94 port codes\n",
    "* Create the 5 dimension tables:\n",
    "    * Create the immigrants table using the i94 immigration data set, with the cicid as the connection to the fact table\n",
    "    * Create the arrivals table using the i94 immigration data set, with the arrdate as the connection to the fact table\n",
    "    * Create the departures table using the i94 immigration data set, with the depdate as the connection to the fact table\n",
    "    * Create the temperatures table using the global temperature data set, with the a joined key of the City and State Code columns as the connection to the fact table\n",
    "    * Create the demographics table using the city demographics data set, with the a joined key of the City and State Code columns as the connection to the fact table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the immigrations fact table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fact_table(immigration_df):\n",
    "    \"\"\" Create immigrations fact table\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    immigration_df: i94 immigration dataframe that has been cleaned and updated\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    fact_table = immigrations table \n",
    "    \"\"\"    \n",
    "    fact_table = immigration_df.join(i94port_df, ['i94port'], how='full').select('visapost','arrdate', 'depdate', 'i94mode', \n",
    "                                                                              col('cicid').alias('id'), \n",
    "                                                                              col('City').alias('City'), \n",
    "                                                                              'State Code', 'i94port','count', \n",
    "                                                                              'visatype', 'entdepa','entdepd',\n",
    "                                                                              'entdepu','admnum')\n",
    "    return fact_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------+--------+------+----------+-------+-----+--------+-------+-------+-------+---------------+\n",
      "|visapost|   arrdate|   depdate|i94mode|      id|  City|State Code|i94port|count|visatype|entdepa|entdepd|entdepu|         admnum|\n",
      "+--------+----------+----------+-------+--------+------+----------+-------+-----+--------+-------+-------+-------+---------------+\n",
      "|     SYD|2016-12-01|2017-01-03|    1.0| 88816.0|Bangor|        ME|    BGM|  1.0|      E2|      G|      R|    N/A|1.5336561285E10|\n",
      "|    null|2016-12-01|2016-12-12|    1.0| 99713.0|Bangor|        ME|    BGM|  1.0|      WT|      G|      O|    N/A|1.5333603185E10|\n",
      "|    null|2016-12-01|2016-12-12|    1.0|113670.0|Bangor|        ME|    BGM|  1.0|      WT|      G|      O|    N/A|1.5332381285E10|\n",
      "|     KWT|2016-12-01|2016-12-03|    1.0|172764.0|Bangor|        ME|    BGM|  1.0|      B1|      G|      R|    N/A|1.5240966785E10|\n",
      "|     KWT|2016-12-01|2016-12-03|    1.0|174077.0|Bangor|        ME|    BGM|  1.0|      B2|      G|      R|    N/A|1.5241767985E10|\n",
      "|     KWT|2016-12-01|2016-12-03|    1.0|181646.0|Bangor|        ME|    BGM|  1.0|      B1|      G|      R|    N/A|1.5242012685E10|\n",
      "|     MEX|2016-12-02|2016-12-17|    1.0|271490.0|Bangor|        ME|    BGM|  1.0|      B2|      G|      I|    N/A|1.5539032885E10|\n",
      "|     MEX|2016-12-02|2016-12-08|    1.0|279585.0|Bangor|        ME|    BGM|  1.0|      B1|      G|      I|    N/A|1.5539972385E10|\n",
      "|    null|2016-12-02|2016-12-04|    1.0|300645.0|Bangor|        ME|    BGM|  1.0|      WB|      G|      R|    N/A|1.5465979085E10|\n",
      "|     LND|2016-12-02|2016-12-04|    1.0|309046.0|Bangor|        ME|    BGM|  1.0|      B1|      G|      R|    N/A|1.5466100085E10|\n",
      "+--------+----------+----------+-------+--------+------+----------+-------+-----+--------+-------+-------+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigrations_table = create_fact_table(immigration_df)\n",
    "immigrations_table.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the immigrants dimension table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_year(year):\n",
    "    current_year = int(datetime.now().strftime('%Y'))\n",
    "    year = int(year)\n",
    "    age = current_year - year\n",
    "    return age\n",
    "\n",
    "def create_immigrants_table(df):\n",
    "    \"\"\" Create immigrans dimenson table\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df: i94 immigration dataframe that has been cleaned and updated\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "     immigrants_df = immigrants table \n",
    "    \"\"\"    \n",
    "    immigrants_df = df.select(col('cicid').alias('id'), 'gender', col('biryear').alias('age'), col('i94visa').alias('visa'), 'occup')\n",
    "\n",
    "    udf_age_converter = udf(lambda x: _convert_year(x), T.IntegerType())\n",
    "    immigrants_df = immigrants_df.withColumn('age', udf_age_converter('age'))\n",
    "\n",
    "    return immigrants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---+--------+-----+\n",
      "|  id|gender|age|    visa|occup|\n",
      "+----+------+---+--------+-----+\n",
      "|46.0|     M| 50|Buisness| null|\n",
      "|56.0|     F| 32| Student| null|\n",
      "|67.0|     M| 52|Pleasure| null|\n",
      "|68.0|     F| 50|Pleasure| null|\n",
      "|69.0|     M| 52|Pleasure| null|\n",
      "|70.0|     F| 13|Pleasure| null|\n",
      "|71.0|     F| 38|Pleasure| null|\n",
      "|72.0|     F| 73|Pleasure| null|\n",
      "|73.0|     F| 70|Pleasure| null|\n",
      "|74.0|     M| 60|Pleasure| null|\n",
      "+----+------+---+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigrants_table = create_immigrants_table(immigration_df)\n",
    "immigrants_table.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use function below to create both the departures and arrivals calendar tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_calendar_table(df, date_column):\n",
    "    \"\"\" Create calendar type dimenson table\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df: i94 immigration dataframe that has been cleaned and updated\n",
    "    date_column: column to be used to extract the date types\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "     calendar_df = calendar type table \n",
    "    \"\"\"    \n",
    "    calendar_df = df.select(date_column).distinct()\n",
    "\n",
    "    calendar_df = calendar_df.withColumn('year', year(date_column))\n",
    "    calendar_df = calendar_df.withColumn('month', month(date_column))\n",
    "    calendar_df = calendar_df.withColumn('day', dayofmonth(date_column))\n",
    "    calendar_df = calendar_df.withColumn('week', weekofyear(date_column))\n",
    "    calendar_df = calendar_df.withColumn('weekday', date_format(date_column,'E'))\n",
    "    \n",
    "    return calendar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the arrival dates dimension table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+-------+\n",
      "|   arrdate|year|month|day|week|weekday|\n",
      "+----------+----+-----+---+----+-------+\n",
      "|2016-12-19|2016|   12| 19|  51|    Mon|\n",
      "|2016-12-12|2016|   12| 12|  50|    Mon|\n",
      "|2016-12-13|2016|   12| 13|  50|    Tue|\n",
      "|2016-12-15|2016|   12| 15|  50|    Thu|\n",
      "|2016-12-20|2016|   12| 20|  51|    Tue|\n",
      "|2016-12-21|2016|   12| 21|  51|    Wed|\n",
      "|2016-12-22|2016|   12| 22|  51|    Thu|\n",
      "|2016-12-03|2016|   12|  3|  48|    Sat|\n",
      "|2016-12-25|2016|   12| 25|  51|    Sun|\n",
      "|2016-12-31|2016|   12| 31|  52|    Sat|\n",
      "+----------+----+-----+---+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrivals_table = create_calendar_table(immigration_df, 'arrdate')\n",
    "arrivals_table.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the departure dates dimension table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+-------+\n",
      "|   depdate|year|month|day|week|weekday|\n",
      "+----------+----+-----+---+----+-------+\n",
      "|2016-03-01|2016|    3|  1|   9|    Tue|\n",
      "|2017-01-06|2017|    1|  6|   1|    Fri|\n",
      "|2017-01-27|2017|    1| 27|   4|    Fri|\n",
      "|2017-02-26|2017|    2| 26|   8|    Sun|\n",
      "|2016-12-19|2016|   12| 19|  51|    Mon|\n",
      "|2017-01-24|2017|    1| 24|   4|    Tue|\n",
      "|2016-11-08|2016|   11|  8|  45|    Tue|\n",
      "|2016-07-03|2016|    7|  3|  26|    Sun|\n",
      "|2017-02-16|2017|    2| 16|   7|    Thu|\n",
      "|2017-04-09|2017|    4|  9|  14|    Sun|\n",
      "+----------+----+-----+---+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_table = create_calendar_table(immigration_df, 'depdate')\n",
    "departures_table.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the temperatures dimension table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_city_temps_table(df, i94port_df):\n",
    "    \"\"\" Create temperatures dimenson table\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df: temperature dataframe that has been cleaned and updated\n",
    "    i94port_df: cleaned dataframe containing the valid i94port values\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    temps_df = temperatures table \n",
    "    \"\"\"    \n",
    "    temps_df = df.join(i94port_df, ['City'], how='left')\n",
    "    temps_df = temps_df.withColumn('AverageTemperature', col('AverageTemperature').cast('double'))\n",
    "    temps_df = temps_df.select(col('dt').alias('Date'), 'City', 'Country', 'State Code', 'AverageTemperature', 'Latitude', 'Longitude')\n",
    "    return temps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+----------+-------------------+--------+---------+\n",
      "|      Date|            City|      Country|State Code| AverageTemperature|Latitude|Longitude|\n",
      "+----------+----------------+-------------+----------+-------------------+--------+---------+\n",
      "|2000-01-01|        Savannah|United States|        GA| 11.484000000000002|  31.35N|   81.05W|\n",
      "|2000-01-01|       Sunnyvale|United States|      null|             10.081|  37.78N|  122.03W|\n",
      "|2000-02-01|Sterling Heights|United States|      null|-0.8250000000000002|  42.59N|   82.91W|\n",
      "|2000-02-01|          Topeka|United States|      null|              4.659|  39.38N|   95.72W|\n",
      "|2000-02-01|          Tucson|United States|        AZ|             12.377|  31.35N|  111.20W|\n",
      "|2000-05-01|        Beaumont|United States|        TX| 25.316999999999997|  29.74N|   94.15W|\n",
      "|2000-05-01|        El Monte|United States|      null|             19.073|  34.56N|  118.70W|\n",
      "|2000-05-01|   Oklahoma City|United States|        OK|             21.844|  36.17N|   97.46W|\n",
      "|2000-05-01|     West Covina|United States|      null|             19.073|  34.56N|  118.70W|\n",
      "|2000-06-01|       Fairfield|United States|      null|             19.724|  37.78N|  122.03W|\n",
      "+----------+----------------+-------------+----------+-------------------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures_table = create_city_temps_table(temperature_df,i94port_df)\n",
    "temperatures_table.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the city demographics table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographics_table(df):\n",
    "    \"\"\" Create demographics dimenson table\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df: demographics dataframe that has been cleaned and updated\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    demo_df = demographics table \n",
    "    \"\"\"    \n",
    "    demo_df = df.withColumn('Male Population', col('Male Population').cast('int'))\n",
    "    demo_df = demo_df.withColumn('Female Population', col('Female Population').cast('int'))\n",
    "    demo_df = demo_df.withColumn('Total Population', col('Total Population').cast('int'))\n",
    "    demo_df = demo_df.withColumn('Foreign-born', col('Foreign-born').cast('int'))\n",
    "    demo_df = demo_df.select('City', 'State Code', 'State', 'Male Population', 'Female Population', \n",
    "                             'Total Population', 'Foreign-born', 'Race')\n",
    "    return demo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+---------------+-----------------+----------------+------------+--------------------+\n",
      "|          City|State Code|      State|Male Population|Female Population|Total Population|Foreign-born|                Race|\n",
      "+--------------+----------+-----------+---------------+-----------------+----------------+------------+--------------------+\n",
      "|  Delray Beach|        FL|    Florida|          32219|            34042|           66261|       16639|               Asian|\n",
      "|   Jersey City|        NJ| New Jersey|         131765|           132512|          264277|      109186|  Hispanic or Latino|\n",
      "|     Rockville|        MD|   Maryland|          31205|            35793|           66998|       25047|American Indian a...|\n",
      "|      Alhambra|        CA| California|          42184|            43388|           85572|       44441|American Indian a...|\n",
      "|    Cincinnati|        OH|       Ohio|         143654|           154883|          298537|       16896|               White|\n",
      "|      Gulfport|        MS|Mississippi|          33108|            38764|           71872|        3072|Black or African-...|\n",
      "|  South Jordan|        UT|       Utah|          33669|            32970|           66639|        6142|               White|\n",
      "|Urban Honolulu|        HI|     Hawaii|         176807|           175959|          352766|      101312|               Asian|\n",
      "|    Boca Raton|        FL|    Florida|          44760|            48466|           93226|       21117|               White|\n",
      "|        Caguas|        PR|Puerto Rico|          34743|            42265|           77008|        null|  Hispanic or Latino|\n",
      "+--------------+----------+-----------+---------------+-----------------+----------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics_table = create_demographics_table(demographics_df)\n",
    "demographics_table.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {'departures table': departures_table,\n",
    "          'demographics table': demographics_table,\n",
    "          'temperatures table': temperatures_table,\n",
    "          'arrivals table': arrivals_table,\n",
    "          'immigrants table': immigrants_table, \n",
    "          'immigrations table': immigrations_table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, table in tables.items():\n",
    "    print(f\"Performing data quality check on table {table_name}...\")\n",
    "    total_count = table.count()\n",
    "    if total_count == 0:\n",
    "        print(f\"Data quality check failed for {table_name} with zero records!\")\n",
    "    else:\n",
    "        for column in table.schema.names:\n",
    "            null_values = table.select(col(f'{column}')).where(col(f'{column}').isNull())\n",
    "            if null_values.first():\n",
    "                print(f\"Found NULL values in {column} column!\")\n",
    "        print(f\"Data quality check passed for {table_name} with {total_count:,} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### Immigration Fact Table \n",
    "\n",
    "| Immigrations Table | |\n",
    "| --- | --- | \n",
    "| id| The immigrants  id (Primary Key)| \n",
    "| arrdate| Date of the immigrant's arrival to the USA (Foreign Key) |\n",
    "| depdate | Date of the immigrant's departure to the USA (Foreign Key)|\n",
    "| i94mode | The way immigrant arrived to the USA. '1' = Air, '2' = Sea, '3' = Land, '9' = Not reported |\n",
    "| City| The city the port of admission is located (Foreign Key)| \n",
    "| State Code| 2 letter abbreviation of the state (Foreign Key)|\n",
    "| i94port| Port of admission | \n",
    "| count| The number of people|\n",
    "| visapost| Department of State where where Visa was issued|\n",
    "| entdepa| Arrival Flag - admitted or paroled into the U.S |\n",
    "| entdepd| Departure Flag - Departed, lost I-94 or is deceased|\n",
    "| admnum| Admission Number|\n",
    "\n",
    "\n",
    "##### Immigrants Table\n",
    "| Immigrants Table | |\n",
    "| --- | --- | \n",
    "| id| The immigrants  id (Primary Key) |\n",
    "| gender| Male or Female | \n",
    "| age | Age of immigrant |\n",
    "| visa | Type of Visa |\n",
    "|occup| Ocupation that will be performed|\n",
    "\n",
    "##### Arrivals Table\n",
    "| Arrivals Table | |\n",
    "| --- | --- | \n",
    "| arrdate| arrival date to the USA\n",
    "| year| year of arrival |\n",
    "| month| month of arrival|\n",
    "| day| day of arrival |\n",
    "| week| week of the year|\n",
    "|weekday| day of the week |\n",
    "\n",
    "##### Departures Table\n",
    "| Departures Table | |\n",
    "| --- | --- | \n",
    "| depdate|departure date to the USA\n",
    "| year| year of departure |\n",
    "| month| month of departure|\n",
    "| day| day of departure |\n",
    "| week| week of the year|\n",
    "|weekday| day of the week |\n",
    "\n",
    "\n",
    "##### Temperatures Table\n",
    "| Temperatures Table | |\n",
    "| --- | --- | \n",
    "| City | City Name |\n",
    "| Date | date in YYYY-MM-DD format |\n",
    "| AverageTemperature | The average temperature on that day |\n",
    "| Country | Country Name |\n",
    "| Latitude | The latitude coordinates for the city |\n",
    "| Longitude| The longitude coordinates for the city | \n",
    "| State Code | The 2 letter abbreviation for the state|\n",
    "\n",
    "##### Demographics Table\n",
    "| Demographics Table | |\n",
    "| --- | --- | \n",
    "| City| City Name |\n",
    "| State Code| The 2 letter abbreviation for the state|    \n",
    "| State| The full state name |\n",
    "| Male Population| The total number of males in the state|\n",
    "| Female Population| The total number of females in the state|\n",
    "| Total Population| The total amount of people in the city | \n",
    "| Foreign-born| The amount of people who were not born in the USA|\n",
    "| Race| The most prominent race in the city|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5:  Project Write Up\n",
    "Consdiering the  size of the immigration dataset for only a month, combined with the temperature, port codes and demographic dataset, Spark seems like the best option for processing. The datasets dont get updated rapidly since they are montly so montly update would suffice and thus no need for the use of Apache Airflow at the moment. \n",
    "\n",
    "Alternate requirement scenarios:\n",
    "* If the data was increased by 100x, I would be store data in an Amazon S3 bucket and load it to our staging tables. The ETL would still happen using Spark, since its well suited for large datasets. \n",
    "* If the data populates a dashboard that must be updated on a daily basis by 7am every day, we would use Apache Airflow to perform the ETL and data qualtiy validation.\n",
    "* If the database needed to be accessed by 100+ people, the data can be stored in a database on a redshift cluster which allows for multiple user access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The whole project is done in this notebook but python scripts (etl.py, utils.py and create_tables.py) have been created for reading the data files from an Amazon S3 bucket and write parquet files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
